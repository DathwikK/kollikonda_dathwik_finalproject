# -*- coding: utf-8 -*-
"""kollikonda_dathwik_finalproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZCayKerzxtRZZyiy_YwnLj2gaETeA9yS
"""

# Import Libraries
# ------------------------------------------
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.utils import shuffle
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from IPython.display import display
from tensorflow.keras import Input

# Load and Preprocess Data
# ------------------------------------------
df = pd.read_csv("winequality-red.csv", sep=';')
df.columns = df.columns.str.strip()

# Convert to binary classification
df['quality_binary'] = df['quality'].apply(lambda x: 1 if x >= 6 else 0)
df.drop('quality', axis=1, inplace=True)


# Features and target
X = df.drop('quality_binary', axis=1)
y = df['quality_binary']

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Shuffle
X_scaled, y = shuffle(X_scaled, y, random_state=42)

# Stratified 10-Fold Cross Validation
# ------------------------------------------
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
folds = []
for train_index, test_index in kf.split(X_scaled, y):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    folds.append((X_train, X_test, y_train, y_test))

# Model Training Functions
# ------------------------------------------
def train_random_forest(X_train, y_train):
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    return model

def train_knn(X_train, y_train):
    model = KNeighborsClassifier(n_neighbors=5)
    model.fit(X_train, y_train)
    return model

def build_gru_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(GRU(32))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
    return model

# Manual Metric Calculation
# ------------------------------------------
def calculate_metrics(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    TN, FP, FN, TP = cm.ravel()

    total = TP + TN + FP + FN
    accuracy = (TP + TN) / total
    error_rate = (FP + FN) / total

    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0
    fnr = FN / (TP + FN) if (TP + FN) > 0 else 0
    tss = recall - fpr
    hss_num = 2 * (TP * TN - FP * FN)
    hss_den = ((TP + FN) * (FN + TN)) + ((TP + FP) * (FP + TN))
    hss = hss_num / hss_den if hss_den != 0 else 0

    return {
        "TP": TP, "TN": TN, "FP": FP, "FN": FN,
        "Accuracy": accuracy,
        "Error Rate": error_rate,
        "Recall": recall,
        "Precision": precision,
        "F1 Score": f1,
        "FPR": fpr,
        "FNR": fnr,
        "TSS": tss,
        "HSS": hss
    }

# Train Models & Collect Metrics
# ------------------------------------------
metrics_rf, metrics_knn, metrics_gru = [], [], []

for fold_idx, (X_train, X_test, y_train, y_test) in enumerate(folds, start=1):
    print(f"\nFold {fold_idx}")

    # Random Forest
    rf_model = train_random_forest(X_train, y_train)
    rf_preds = rf_model.predict(X_test)
    metrics_rf.append(calculate_metrics(y_test, rf_preds))

    # KNN
    knn_model = train_knn(X_train, y_train)
    knn_preds = knn_model.predict(X_test)
    metrics_knn.append(calculate_metrics(y_test, knn_preds))

    # GRU
    X_train_gru = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
    X_test_gru = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
    gru_model = build_gru_model((1, X_train.shape[1]))
    es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)
    gru_model.fit(X_train_gru, y_train, epochs=20, batch_size=32,
                  validation_split=0.1, callbacks=[es], verbose=0)
    gru_preds = (gru_model.predict(X_test_gru) > 0.5).astype("int32").flatten()
    metrics_gru.append(calculate_metrics(y_test, gru_preds))

# Summarize Results
# ------------------------------------------
def summarize_metrics(metrics_list):
    df = pd.DataFrame(metrics_list)
    df.loc['Average'] = df.mean(numeric_only=True)
    df.index = [f'Fold {i+1}' for i in range(len(metrics_list))] + ['Average']
    return df.round(4)
print("\nðŸ“Š Summary for Random Forest")
df_rf = summarize_metrics(metrics_rf)
display(df_rf)

print("\nðŸ“Š Summary for KNN")
df_knn = summarize_metrics(metrics_knn)
display(df_knn)

print("\nðŸ“Š Summary for GRU")
df_gru = summarize_metrics(metrics_gru)
display(df_gru)

"""***Discussion***

**Random Forest outperformed KNN and GRU across all evaluation metrics.
This is expected as Random Forest handles noisy tabular data well, builds multiple trees, and reduces overfitting through ensembling.
KNN was fast and easy to train but showed weakness in precision due to class overlap and sensitivity to feature scale.
GRU, while competitive, is primarily designed for sequential data (e.g., time-series) and does not have a structural advantage over Random Forest in this tabular, non-temporal context.**
"""